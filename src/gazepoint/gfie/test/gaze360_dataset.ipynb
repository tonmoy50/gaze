{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import Image as IPImage\n",
    "# import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional  as TF\n",
    "\n",
    "# from utils import img_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tonmoy/Library/CloudStorage/OneDrive-IndianaUniversity/Research/Education Project/Gaze/src/gazepoint/gfie/test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUR_DIR = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "CUR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tonmoy/Library/CloudStorage/OneDrive-IndianaUniversity/Research/Education Project/Data/gaze360'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORKSPACE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(CUR_DIR)))))\n",
    "DATASET_DIR = os.path.join(WORKSPACE_DIR, \"Data\", \"gaze360\")\n",
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'recordings', 'recording', 'frame', 'ts', 'target_cam', 'target_pos3d', 'target_pos2d', 'person_identity', 'person_cam', 'person_eyes3d', 'person_eyes2d', 'person_body_bbox', 'person_head_bbox', 'person_face_bbox', 'person_eye_left_bbox', 'person_eye_right_bbox', 'gaze_dir', 'splits', 'split'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = loadmat(os.path.join(CUR_DIR, \"metadata.mat\"))\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array(['rec_000'], dtype='<U7'), array(['rec_001'], dtype='<U7'),\n",
       "        array(['rec_002'], dtype='<U7'), array(['rec_003'], dtype='<U7'),\n",
       "        array(['rec_004'], dtype='<U7'), array(['rec_005'], dtype='<U7'),\n",
       "        array(['rec_006'], dtype='<U7'), array(['rec_007'], dtype='<U7'),\n",
       "        array(['rec_008'], dtype='<U7'), array(['rec_009'], dtype='<U7'),\n",
       "        array(['rec_010'], dtype='<U7'), array(['rec_011'], dtype='<U7'),\n",
       "        array(['rec_012'], dtype='<U7'), array(['rec_013'], dtype='<U7'),\n",
       "        array(['rec_014'], dtype='<U7'), array(['rec_015'], dtype='<U7'),\n",
       "        array(['rec_016'], dtype='<U7'), array(['rec_017'], dtype='<U7'),\n",
       "        array(['rec_018'], dtype='<U7'), array(['rec_019'], dtype='<U7'),\n",
       "        array(['rec_020'], dtype='<U7'), array(['rec_021'], dtype='<U7'),\n",
       "        array(['rec_022'], dtype='<U7'), array(['rec_023'], dtype='<U7'),\n",
       "        array(['rec_024'], dtype='<U7'), array(['rec_025'], dtype='<U7'),\n",
       "        array(['rec_026'], dtype='<U7'), array(['rec_027'], dtype='<U7'),\n",
       "        array(['rec_028'], dtype='<U7'), array(['rec_029'], dtype='<U7'),\n",
       "        array(['rec_030'], dtype='<U7'), array(['rec_031'], dtype='<U7'),\n",
       "        array(['rec_032'], dtype='<U7'), array(['rec_033'], dtype='<U7'),\n",
       "        array(['rec_034'], dtype='<U7'), array(['rec_035'], dtype='<U7'),\n",
       "        array(['rec_036'], dtype='<U7'), array(['rec_037'], dtype='<U7'),\n",
       "        array(['rec_038'], dtype='<U7'), array(['rec_039'], dtype='<U7'),\n",
       "        array(['rec_040'], dtype='<U7'), array(['rec_041'], dtype='<U7'),\n",
       "        array(['rec_042'], dtype='<U7'), array(['rec_043'], dtype='<U7'),\n",
       "        array(['rec_044'], dtype='<U7'), array(['rec_045'], dtype='<U7'),\n",
       "        array(['rec_046'], dtype='<U7'), array(['rec_047'], dtype='<U7'),\n",
       "        array(['rec_048'], dtype='<U7'), array(['rec_049'], dtype='<U7'),\n",
       "        array(['rec_050'], dtype='<U7'), array(['rec_051'], dtype='<U7'),\n",
       "        array(['rec_052'], dtype='<U7'), array(['rec_053'], dtype='<U7'),\n",
       "        array(['rec_054'], dtype='<U7'), array(['rec_055'], dtype='<U7'),\n",
       "        array(['rec_056'], dtype='<U7'), array(['rec_057'], dtype='<U7'),\n",
       "        array(['rec_058'], dtype='<U7'), array(['rec_059'], dtype='<U7'),\n",
       "        array(['rec_060'], dtype='<U7'), array(['rec_061'], dtype='<U7'),\n",
       "        array(['rec_062'], dtype='<U7'), array(['rec_063'], dtype='<U7'),\n",
       "        array(['rec_064'], dtype='<U7'), array(['rec_065'], dtype='<U7'),\n",
       "        array(['rec_066'], dtype='<U7'), array(['rec_067'], dtype='<U7'),\n",
       "        array(['rec_068'], dtype='<U7'), array(['rec_069'], dtype='<U7'),\n",
       "        array(['rec_070'], dtype='<U7'), array(['rec_071'], dtype='<U7'),\n",
       "        array(['rec_072'], dtype='<U7'), array(['rec_073'], dtype='<U7'),\n",
       "        array(['rec_074'], dtype='<U7'), array(['rec_075'], dtype='<U7'),\n",
       "        array(['rec_076'], dtype='<U7'), array(['rec_077'], dtype='<U7'),\n",
       "        array(['rec_078'], dtype='<U7'), array(['rec_079'], dtype='<U7')]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"recordings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 79, 79, 79], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"recording\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2, ..., 521, 522, 522]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.12525296,  0.25050497, ..., 63.37850809,\n",
       "        63.50351596, 63.50351596]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"person_identity\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69389339,  1.75942589, -0.62393989],\n",
       "       [ 1.68793889,  1.76528217, -0.62475749],\n",
       "       [ 1.68868502,  1.76575734, -0.62507184],\n",
       "       ...,\n",
       "       [ 0.67011673, -0.48928031, -0.51240202],\n",
       "       [ 0.64817876, -0.47156056, -0.44699191],\n",
       "       [ 0.64817876, -0.47156056, -0.44699191]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"target_pos3d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58718794, 0.4859066 , 0.04633445, 0.0382576 ],\n",
       "       [0.5837803 , 0.48372877, 0.04876879, 0.04026759],\n",
       "       [0.5783111 , 0.47928429, 0.05532767, 0.04568315],\n",
       "       ...,\n",
       "       [0.51265723, 0.47321415, 0.08451395, 0.06978178],\n",
       "       [0.49715102, 0.47933304, 0.08145172, 0.06725335],\n",
       "       [0.51202176, 0.47195637, 0.08704371, 0.07187057]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"person_head_bbox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(mat, dstype):\n",
    "    orig_x, orig_y = 3382, 4096\n",
    "    frame = mat[\"frame\"]\n",
    "    # person_eyes_2d = mat[\"person_eyes2d\"]\n",
    "    # person_eyes_3d = mat[\"person_eyes3d\"]\n",
    "\n",
    "    himg = list()\n",
    "    simg = list()\n",
    "    frame_id = list()\n",
    "    hbbox = list()\n",
    "    fbbox = list()\n",
    "    bbbox = list()\n",
    "    person_eyes2d = list()\n",
    "    person_eyes3d = list()\n",
    "    gaze_direction = list()\n",
    "    target2d = list()\n",
    "    target3d = list()\n",
    "\n",
    "\n",
    "    for i in range(len(frame[0])):\n",
    "        if  mat[\"split\"][0][i] == dstype:\n",
    "            himg += [os.path.join(\n",
    "                mat[\"recordings\"][0][mat[\"recording\"][0][i]].item(),\n",
    "                \"head\",\n",
    "                \"%06d\" % mat[\"person_identity\"][0][i].item(),\n",
    "                \"%06d.jpg\" % mat[\"frame\"][0][i].item(),\n",
    "            )]\n",
    "            simg += [os.path.join(\n",
    "                mat[\"recordings\"][0][mat[\"recording\"][0][i]].item(),\n",
    "                \"body\",\n",
    "                \"%06d\" % mat[\"person_identity\"][0][i].item(),\n",
    "                \"%06d.jpg\" % mat[\"frame\"][0][i].item(),\n",
    "            )]\n",
    "            frame_id += [mat[\"frame\"][0][i]]\n",
    "            hbbox += [mat[\"person_head_bbox\"][i]]\n",
    "            # fbbox += [mat[\"person_face_bbox\"][i]]\n",
    "            # bbbox += [mat[\"person_body_bbox\"][i]]\n",
    "            person_eyes2d += [mat[\"person_eyes2d\"][i]]\n",
    "            person_eyes3d += [mat[\"person_eyes3d\"][i]]\n",
    "            gaze_direction += [mat[\"gaze_dir\"][i]]\n",
    "            target2d += [mat[\"target_pos2d\"][i]]\n",
    "            target3d += [mat[\"target_pos3d\"][i]]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"simg\": simg,\n",
    "        \"himg\": himg,\n",
    "        \"frame_id\": frame_id,\n",
    "        \"h_x_min\" : [x[0] for x in hbbox],\n",
    "        \"h_y_min\" : [x[1] for x in hbbox],\n",
    "        \"h_x_max\" : [x[2] for x in hbbox],\n",
    "        \"h_y_max\" : [x[3] for x in hbbox],\n",
    "        \"eye_u\" : [x[0] for x in person_eyes2d],\n",
    "        \"eye_v\" : [x[1] for x in person_eyes2d],\n",
    "        \"eye_X\" : [x[0] for x in person_eyes3d],\n",
    "        \"eye_Y\" : [x[1] for x in person_eyes3d],\n",
    "        \"eye_Z\" : [x[2] for x in person_eyes3d],\n",
    "        \"gaze_dirX\": [x[0] for x in gaze_direction],\n",
    "        \"gaze_dirY\": [x[1] for x in gaze_direction],\n",
    "        \"gaze_dirZ\": [x[2] for x in gaze_direction],\n",
    "        \"gaze_u\" : [x[0] for x in target2d],\n",
    "        \"gaze_v\": [x[1] for x in target2d],\n",
    "        \"gaze_X\": [x[0] for x in target3d],\n",
    "        \"gaze_Y\": [x[1] for x in target3d],\n",
    "        \"gaze_Z\": [x[2] for x in target3d],\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126928, 20)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare Train Dataset\n",
    "df = prepare_dataset(mat, 0)\n",
    "df.to_csv(os.path.join(DATASET_DIR, \"train.csv\"))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17038, 20)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare Validation Dataset\n",
    "df = prepare_dataset(mat, 1)\n",
    "df.to_csv(os.path.join(DATASET_DIR, \"validation.csv\"))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25969, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare Test Dataset\n",
    "df = prepare_dataset(mat, 2)\n",
    "df.to_csv(os.path.join(DATASET_DIR, \"test.csv\"))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = os.path.join(DATASET_DIR, \"imgs\", mat[\"recordings\"]) # , \"body\", '%06d' % mat[\"person_identity\"][0], \"%06d.jpg\" % mat[\"frame\"][0]\n",
    "# img_path\n",
    "orig_x, orig_y = 3382, 4096\n",
    "recordings = mat[\"recordings\"]\n",
    "recording = mat[\"recording\"]\n",
    "splits = mat[\"splits\"]\n",
    "split = mat[\"split\"]\n",
    "person_head_bbox = mat[\"person_head_bbox\"]\n",
    "person_face_bbox = mat[\"person_face_bbox\"]\n",
    "person_body_bbox = mat[\"person_body_bbox\"]\n",
    "person_identity = mat[\"person_identity\"]\n",
    "gaze_dir = mat[\"gaze_dir\"]\n",
    "frame = mat[\"frame\"]\n",
    "person_eyes_2d = mat[\"person_eyes_2d\"]\n",
    "person_eyes_3d = mat[\"person_eyes_3d\"]\n",
    "\n",
    "i = 7370\n",
    "img_path = os.path.join(\n",
    "    DATASET_DIR,\n",
    "    \"imgs\",\n",
    "    recordings[0][recording[0][i]].item(),\n",
    "    \"head\",\n",
    "    \"%06d\" % mat[\"person_identity\"][0][i].item(),\n",
    "    \"%06d.jpg\" % mat[\"frame\"][0][i].item(),\n",
    ")\n",
    "# img_path\n",
    "# img = Image.open(img_path)\n",
    "# Image._show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"vinvino02/glpn-nyu\"\n",
    "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint, device=0)\n",
    "\n",
    "sample_img = Image.open(os.path.join(DATASET_DIR, \"imgs\", lines[0].split(\" \")[0]))\n",
    "preds = depth_estimator(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 217)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_depth': tensor([[[5.3033, 5.9054, 5.7112,  ..., 4.9678, 5.0379, 5.1203],\n",
       "          [6.2931, 7.2684, 7.4032,  ..., 4.8657, 5.0426, 4.9449],\n",
       "          [6.5597, 7.6745, 7.8852,  ..., 5.2611, 5.4131, 5.0675],\n",
       "          ...,\n",
       "          [3.2218, 3.0695, 2.8769,  ..., 5.6232, 5.4847, 5.1200],\n",
       "          [3.4083, 3.1831, 2.9930,  ..., 5.5539, 5.3422, 5.0289],\n",
       "          [3.8051, 3.6194, 3.3994,  ..., 5.2701, 5.1984, 4.9153]]]),\n",
       " 'depth': <PIL.Image.Image image mode=L size=217x217>}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 192, 192]), (217, 217))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[\"predicted_depth\"].shape, preds[\"depth\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaze360(Dataset):\n",
    "\n",
    "    def __init__(self,dstype,opt,show=False):\n",
    "        self.dataset_root = opt.DATASET.root_dir\n",
    "        # rgb_path=os.path.join(opt.DATASET.root_dir,opt.DATASET.rgb)\n",
    "        # depth_path=os.path.join(opt.DATASET.root_dir,opt.DATASET.depth)\n",
    "\n",
    "        # camerapara=np.load(os.path.join(opt.DATASET.root_dir,opt.DATASET.camerapara))\n",
    "\n",
    "        if dstype==\"train\":\n",
    "            annofile_path=os.path.join(opt.DATASET.root_dir,opt.DATASET.train)\n",
    "        elif dstype==\"valid\":\n",
    "            annofile_path = os.path.join(opt.DATASET.root_dir, opt.DATASET.valid)\n",
    "        elif dstype==\"test\":\n",
    "            annofile_path = os.path.join(opt.DATASET.root_dir, opt.DATASET.test)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        df=pd.read_csv(annofile_path)\n",
    "\n",
    "        self.X_train = df[['simg', 'himg', \"h_x_min\",\"h_y_min\",\"h_x_max\",\"h_y_max\",'eye_u','eye_v','eye_X','eye_Y','eye_Z']]\n",
    "        self.Y_train = df[['gaze_u', 'gaze_v', 'gaze_X', 'gaze_Y', 'gaze_Z']]\n",
    "\n",
    "        self.length=len(df)\n",
    "\n",
    "        # self.rgb_path=rgb_path\n",
    "        # self.depth_path=depth_path\n",
    "        # self.camerapara=camerapara\n",
    "\n",
    "        self.input_size=opt.TRAIN.input_size\n",
    "        self.output_size=opt.TRAIN.output_size\n",
    "\n",
    "        transform_list = []\n",
    "        transform_list.append(transforms.Resize((self.input_size, self.input_size)))\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "        transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "        self.dstype=dstype\n",
    "\n",
    "        self.imshow=show\n",
    "\n",
    "        self.depth_estimator = pipeline(\"depth-estimation\", model=\"vinvino02/glpn-nyu\", device=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # scene_id,frame_index,h_x_min,h_y_min,h_x_max,h_y_max,eye_u,eye_v,eye_X,eye_Y,eye_Z=self.X_train.iloc[index]\n",
    "        simg, himg, h_x_min, h_y_min, h_x_max, h_y_max, eye_u, eye_v, eye_X, eye_Y, eye_Z = self.X_train.iloc[index]\n",
    "        # scene_id=str(int(scene_id))\n",
    "        # frame_index=int(frame_index)\n",
    "\n",
    "        gaze_u, gaze_v,gaze_X, gaze_Y, gaze_Z = self.Y_train.iloc[index]\n",
    "\n",
    "        # rgb_path=os.path.join(self.rgb_path,self.dstype,\"scene{}\".format(scene_id),\"{:04}.jpg\".format(frame_index))\n",
    "        # depth_path=os.path.join(self.depth_path,self.dstype,\"scene{}\".format(scene_id),\"{:04}.npy\".format(frame_index))\n",
    "\n",
    "        # load the rgb image\n",
    "        img = Image.open(os.path.join(self.dataset_root, \"imgs\", simg))\n",
    "        img = img.convert('RGB')\n",
    "        width, height = img.size\n",
    "        org_width, org_height = width, height\n",
    "\n",
    "        # load the depth image\n",
    "        # depthimg=np.load(depth_path)\n",
    "        # # replace the invalid value with 0\n",
    "        # depthimg[np.isnan(depthimg)]=0\n",
    "        # depthimg=depthimg.astype(np.float32)\n",
    "        # depthimg=Image.fromarray(depthimg)\n",
    "        depthimg = depth_estimator(Image.open(os.path.join(self.dataset_root, \"imgs\", simg)))[\"depth\"]\n",
    "        depth_estimator = pipeline(\"depth-estimation\", model=\"vinvino02/glpn-nyu\", device=0)\n",
    "        estimated_depth = depth_estimator(img)\n",
    "        depthimg = estimated_depth[\"depth\"]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # expand face bbox a bit\n",
    "        k=0.1\n",
    "        h_x_min -= k * abs(h_x_max - h_x_min)\n",
    "        h_y_min -= k * abs(h_y_max - h_y_min)\n",
    "        h_x_max += k * abs(h_x_max - h_x_min)\n",
    "        h_y_max += k * abs(h_y_max - h_y_min)\n",
    "\n",
    "        x_min, y_min, x_max, y_max=map(float,[h_x_min,h_y_min,h_x_max,h_y_max])\n",
    "\n",
    "        # Data augmentation for training procedure\n",
    "        offset_x, offset_y = 0, 0\n",
    "        flip_flag = False\n",
    "        if self.dstype==\"train\":\n",
    "\n",
    "            # Jitter (expansion-only) bounding box size\n",
    "            if np.random.random_sample() <= 0.5:\n",
    "                k = np.random.random_sample() * 0.2\n",
    "                x_min -= k * abs(x_max - x_min)\n",
    "                y_min -= k * abs(y_max - y_min)\n",
    "                x_max += k * abs(x_max - x_min)\n",
    "                y_max += k * abs(y_max - y_min)\n",
    "\n",
    "            # Random crop\n",
    "            if np.random.random_sample() <= 0.5:\n",
    "                # calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n",
    "                crop_x_min = np.min([gaze_u , x_min, x_max])\n",
    "                crop_y_min = np.min([gaze_v , y_min, y_max])\n",
    "                crop_x_max = np.max([gaze_u , x_min, x_max])\n",
    "                crop_y_max = np.max([gaze_v , y_min, y_max])\n",
    "\n",
    "                # randomly select a top left corner\n",
    "                if crop_x_min >= 0:\n",
    "                    crop_x_min = np.random.uniform(0, crop_x_min)\n",
    "                if crop_y_min >= 0:\n",
    "                    crop_y_min = np.random.uniform(0, crop_y_min)\n",
    "\n",
    "                # find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n",
    "                crop_width_min = crop_x_max - crop_x_min\n",
    "                crop_height_min = crop_y_max - crop_y_min\n",
    "                crop_width_max = width - crop_x_min\n",
    "                crop_height_max = height - crop_y_min\n",
    "                # randomly select a width and a height\n",
    "                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n",
    "                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n",
    "\n",
    "                # crop scene img\n",
    "                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n",
    "\n",
    "                # crop depth img\n",
    "                depthimg=TF.crop(depthimg, crop_y_min, crop_x_min, crop_height, crop_width)\n",
    "\n",
    "                # record the crop's (x, y) offset\n",
    "                offset_x, offset_y = crop_x_min, crop_y_min\n",
    "\n",
    "                # convert coordinates into the cropped frame\n",
    "                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n",
    "\n",
    "                gaze_u, gaze_v = (gaze_u - offset_x) / float(crop_width), \\\n",
    "                                 (gaze_v - offset_y) / float(crop_height)\n",
    "\n",
    "                eye_u, eye_v = (eye_u - offset_x) / float(crop_width), \\\n",
    "                                 (eye_v  - offset_y) / float(crop_height)\n",
    "\n",
    "                width, height = crop_width, crop_height\n",
    "\n",
    "            else:\n",
    "                gaze_u, gaze_v = (gaze_u - offset_x) / float(width), \\\n",
    "                                 (gaze_v - offset_y) / float(height)\n",
    "\n",
    "                eye_u, eye_v = (eye_u - offset_x) / float(width), \\\n",
    "                                 (eye_v  - offset_y) / float(height)\n",
    "\n",
    "            # Random flip\n",
    "            if np.random.random_sample() <= 0.5:\n",
    "                flip_flag=True\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                depthimg = depthimg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                x_max_2 = width - x_min\n",
    "                x_min_2 = width - x_max\n",
    "                x_max = x_max_2\n",
    "                x_min = x_min_2\n",
    "                gaze_u= 1 - gaze_u\n",
    "                eye_u=1 -eye_u\n",
    "\n",
    "            # Random change the brightness, contrast and saturation of the scene images\n",
    "            if np.random.random_sample() <= 0.5:\n",
    "                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n",
    "                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n",
    "                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n",
    "\n",
    "        else:\n",
    "\n",
    "            gaze_u, gaze_v = gaze_u  / float(width), \\\n",
    "                             gaze_v  / float(height)\n",
    "\n",
    "            eye_u, eye_v = eye_u  / float(width), \\\n",
    "                           eye_v  / float(height)\n",
    "\n",
    "        # represent the head location with mask\n",
    "        head_channel = img_utils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n",
    "                                                    resolution=self.input_size, coordconv=False).unsqueeze(0)\n",
    "\n",
    "        # the final image size in train/valid/test\n",
    "        final_width,final_height=img.size\n",
    "\n",
    "        # crop the face\n",
    "        headimg = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n",
    "\n",
    "        # set for display\n",
    "        if self.imshow:\n",
    "            img_show=img\n",
    "            depthimg_show=depthimg\n",
    "\n",
    "        # resize scene/face image and convert them to tensor\n",
    "        if self.transform is not None:\n",
    "\n",
    "            img=self.transform(img)\n",
    "            headimg=self.transform(headimg)\n",
    "\n",
    "        # Generate the matrix_T\n",
    "        depthmap=depthimg.resize((self.input_size,self.input_size),Image.BICUBIC)\n",
    "        depthmap=np.array(depthmap)\n",
    "\n",
    "        # scale proportionally\n",
    "        scale_width,scale_height=final_width/self.input_size,final_height/self.input_size\n",
    "\n",
    "        # construct empty matrix\n",
    "        matrix_T_DW = np.linspace(0, self.input_size - 1, self.input_size)\n",
    "        matrix_T_DH = np.linspace(0, self.input_size - 1, self.input_size)\n",
    "        [matrix_T_xx, matrix_T_yy] = np.meshgrid(matrix_T_DW, matrix_T_DH)\n",
    "\n",
    "        # construct matrix_T according to Eq 3. in paper\n",
    "        fx,fy,cx,cy=self.camerapara\n",
    "        if flip_flag:\n",
    "            cx= org_width - cx\n",
    "            matrix_T_X = (matrix_T_xx * scale_width + (org_width - final_width - offset_x) - cx) * depthmap / fx\n",
    "\n",
    "        else:\n",
    "            matrix_T_X = (matrix_T_xx * scale_width + offset_x - cx) * depthmap / fx\n",
    "\n",
    "        matrix_T_Y = (matrix_T_yy * scale_height + offset_y - cy) * depthmap / fy\n",
    "        matrix_T_Z = depthmap\n",
    "\n",
    "        matrix_T = np.dstack((matrix_T_X, matrix_T_Y, matrix_T_Z))\n",
    "        matrix_T = matrix_T.reshape([-1, 3])\n",
    "        matrix_T = matrix_T.reshape([self.input_size, self.input_size, 3])\n",
    "\n",
    "        if flip_flag:\n",
    "            matrix_T = matrix_T - np.array([-eye_X, eye_Y, eye_Z])\n",
    "        else:\n",
    "            matrix_T = matrix_T - np.array([eye_X, eye_Y, eye_Z])\n",
    "\n",
    "        norm_value = np.linalg.norm(matrix_T, axis=2, keepdims=True)\n",
    "        norm_value[norm_value <= 0] = 1\n",
    "\n",
    "        matrix_T = matrix_T / norm_value\n",
    "\n",
    "        # convert it to tensor\n",
    "        matrix_T=torch.from_numpy(matrix_T).float()\n",
    "\n",
    "\n",
    "        # generate the gaze vector label\n",
    "        gaze_vector = np.array([gaze_X - eye_X, gaze_Y - eye_Y, gaze_Z - eye_Z])\n",
    "\n",
    "        if flip_flag:\n",
    "            gaze_vector[0]=-gaze_vector[0]\n",
    "\n",
    "        norm_gaze_vector = 1.0 if np.linalg.norm (gaze_vector) <= 0.0 else np.linalg.norm (gaze_vector)\n",
    "        gaze_vector=gaze_vector/norm_gaze_vector\n",
    "        gaze_vector=torch.from_numpy(gaze_vector)\n",
    "\n",
    "        # generate the heat map label\n",
    "        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n",
    "\n",
    "        gaze_heatmap = img_utils.draw_labelmap(gaze_heatmap, [gaze_u * self.output_size, gaze_v * self.output_size],\n",
    "                                               3,type='Gaussian')\n",
    "\n",
    "\n",
    "        # auxilary info\n",
    "        gaze_target2d=torch.from_numpy(np.array([gaze_u,gaze_v]))\n",
    "        matrix_T_heatmap = np.dot(matrix_T, gaze_vector)\n",
    "\n",
    "        # display\n",
    "        if self.imshow:\n",
    "\n",
    "            def unnorm(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "                std = np.array(std).reshape(3, 1, 1)\n",
    "                mean = np.array(mean).reshape(3, 1, 1)\n",
    "                return img * std + mean\n",
    "\n",
    "            figure,ax=plt.subplots(2,3)\n",
    "            figure.set_size_inches(15 ,8)\n",
    "\n",
    "            simgshow=unnorm(img.numpy()) * 255\n",
    "            simgshow=np.clip(simgshow,0,255)\n",
    "            simgshow=simgshow.astype(np.uint8)\n",
    "\n",
    "            himgshow=unnorm(headimg.numpy()) * 255\n",
    "            himgshow=np.clip(himgshow,0,255)\n",
    "            himgshow=himgshow.astype(np.uint8)\n",
    "\n",
    "            eyes_outpix=[eye_u*self.input_size,eye_v*self.input_size]\n",
    "\n",
    "            gaze_outpix = [gaze_u * self.input_size, gaze_v * self.input_size]\n",
    "\n",
    "            # display scene image\n",
    "            ax[0][0].imshow(np.transpose(simgshow, (1, 2, 0)))\n",
    "            # display gaze target and eyes in scene image\n",
    "            ax[0][0].scatter(eyes_outpix[0],eyes_outpix[1])\n",
    "            ax[0][0].scatter(gaze_outpix[0],gaze_outpix[1])\n",
    "\n",
    "            # display depth map\n",
    "            ax[0][1].imshow(depthmap,cmap='gray')\n",
    "            # display head image\n",
    "            ax[1][0].imshow(np.transpose(himgshow, (1, 2, 0)))\n",
    "            # display expected stero FoV heatmap\n",
    "            ax[1][1].imshow(matrix_T_heatmap, cmap='jet')\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        all_data={}\n",
    "        all_data['simg'] = img\n",
    "        all_data[\"himg\"] = headimg\n",
    "        all_data[\"headloc\"] = head_channel\n",
    "        all_data[\"matrixT\"]=matrix_T\n",
    "\n",
    "        # Y_label\n",
    "        all_data[\"gaze_heatmap\"] = gaze_heatmap\n",
    "        all_data[\"gaze_vector\"] = gaze_vector\n",
    "        all_data[\"gaze_target2d\"] = gaze_target2d\n",
    "\n",
    "        return all_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
